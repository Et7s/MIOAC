# –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–∞–∫–µ—Ç—ã
import importlib
def check_module(module_name):
    try:
        importlib.import_module(module_name)
        print(f"‚úÖ {module_name} —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        return True
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ {module_name}: {e}")
        return False

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥—É–ª–µ–π
critical_modules = ['torch', 'torchaudio', 'transformers', 'soundfile', 'librosa', 'gtts']
for module in critical_modules:
    check_module(module)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

import torch
import torchaudio
import soundfile as sf
import librosa
import numpy as np
import matplotlib.pyplot as plt
import time
import gc
import psutil
import os
from pathlib import Path
import IPython.display as ipd
from scipy.io import wavfile
import requests
import tempfile
from gtts import gTTS
import io

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
def measure_resource_usage():
    """–ò–∑–º–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
    cpu_usage = psutil.cpu_percent()
    memory_usage = psutil.virtual_memory().percent
    
    if torch.cuda.is_available():
        gpu_usage = torch.cuda.memory_allocated() / 1024**3  # GB
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    else:
        gpu_usage = 0
        gpu_memory = 0
        
    return {
        'cpu_usage': cpu_usage,
        'memory_usage': memory_usage,
        'gpu_usage_gb': gpu_usage,
        'gpu_memory_gb': gpu_memory
    }

def calculate_mos(audio_path):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É MOS (Mean Opinion Score)
    """
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        audio, sr = librosa.load(audio_path, sr=22050)
        
        # –†–∞—Å—á–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
        rms = librosa.feature.rms(y=audio)[0]
        rms_mean = np.mean(rms)
        
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        zcr_mean = np.mean(zcr)
        
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_centroid_mean = np.mean(spectral_centroids)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è MOS (—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥)
        mos_approximation = (
            0.4 * min(rms_mean / 0.1, 1.0) + 
            0.3 * min(spectral_centroid_mean / 4000, 1.0) +
            0.3 * (1 - min(zcr_mean / 0.1, 1.0))
        ) * 4 + 1 
        
        return min(mos_approximation, 5.0)
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ MOS –¥–ª—è {audio_path}: {e}")
        return 3.0  # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

def get_audio_length(audio_path):
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞"""
    try:
        audio, sr = librosa.load(audio_path, sr=None)
        return len(audio) / sr
    except:
        return 0

# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (—Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
test_texts = [
    "Hello world! This is a text to speech test.",
    "The weather is beautiful today for a walk in the park.",
    "Neural networks are revolutionizing natural language processing.",
    "Machine learning enables computers to learn from data.",
    "Speech synthesis is becoming more natural and expressive.",
    "Artificial intelligence is transforming our world.",
    "Deep learning models require large amounts of training data.",
    "The quick brown fox jumps over the lazy dog.",
    "Text to speech technology has improved significantly in recent years.",
    "This is a demonstration of modern speech synthesis quality."
]

print("–¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã:")
for i, text in enumerate(test_texts, 1):
    print(f"{i}. {text}")

def test_bark():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Bark - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï BARK")
    print("=" * 50)
    
    try:
        from transformers import BarkModel, AutoProcessor
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É —Å float16
        start_time = time.time()
        
        # –£–±–∏—Ä–∞–µ–º torch_dtype=torch.float16 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏
        model = BarkModel.from_pretrained("suno/bark-small")
        processor = AutoProcessor.from_pretrained("suno/bark-small")
        
        model = model.to(device)
        load_time = time.time() - start_time
        
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        results = []
        output_dir = Path("bark_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            # –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–æ —Å–∏–Ω—Ç–µ–∑–∞
            resources_before = measure_resource_usage()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            inputs = processor(text, return_tensors="pt").to(device)
            
            synth_start = time.time()
            with torch.no_grad():
                audio_array = model.generate(**inputs, do_sample=True)
            
            synth_time = time.time() - synth_start
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
            output_path = output_dir / f"bark_{i+1}.wav"
            audio_data = audio_array[0].cpu().numpy()
            sample_rate = model.generation_config.sample_rate
            sf.write(str(output_path), audio_data, sample_rate)
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'Bark',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': len(audio_data) / sample_rate
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del inputs, audio_array
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∏–Ω—Ç–µ–∑–∞–º–∏
            time.sleep(1)
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ Bark: {e}")
        return []


def test_mms_tts():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ MMS TTS"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï MMS TTS")
    print("=" * 50)
    
    try:
        from transformers import VitsModel, AutoTokenizer
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        start_time = time.time()
        model = VitsModel.from_pretrained("facebook/mms-tts-eng")
        tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-eng")
        load_time = time.time() - start_time
        
        model = model.to(device)
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        results = []
        output_dir = Path("mms_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            resources_before = measure_resource_usage()
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            inputs = tokenizer(text, return_tensors="pt").to(device)
            
            synth_start = time.time()
            with torch.no_grad():
                output = model(**inputs)
                
            synth_time = time.time() - synth_start
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞—É–¥–∏–æ
            audio = output.waveform[0].cpu().numpy()
            sample_rate = model.config.sampling_rate
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
            output_path = output_dir / f"mms_{i+1}.wav"
            sf.write(str(output_path), audio, sample_rate)
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'MMS TTS',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': len(audio) / sample_rate
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            del inputs, output
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            time.sleep(0.5)
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ MMS TTS: {e}")
        return []

def test_speecht5():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ SpeechT5"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï SPEECHT5")
    print("=" * 50)
    
    try:
        from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
        from datasets import load_dataset
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        start_time = time.time()
        processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
        model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
        vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
        load_time = time.time() - start_time
        
        model = model.to(device)
        vocoder = vocoder.to(device)
        
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≥–æ–ª–æ—Å–∞
        embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
        speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0).to(device)
        
        results = []
        output_dir = Path("speecht5_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            resources_before = measure_resource_usage()
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            inputs = processor(text=text, return_tensors="pt").to(device)
            
            synth_start = time.time()
            with torch.no_grad():
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã
                spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –≤–æ–∫–æ–¥–µ—Ä–∞
                audio = vocoder(spectrogram)
                
            synth_time = time.time() - synth_start
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
            output_path = output_dir / f"speecht5_{i+1}.wav"
            audio_data = audio[0].cpu().numpy()
            sample_rate = 16000
            sf.write(str(output_path), audio_data, sample_rate)
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'SpeechT5',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': len(audio_data) / sample_rate
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            del inputs, spectrogram, audio
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            time.sleep(0.5)
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ SpeechT5: {e}")
        return []


def test_gtts():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Google Text-to-Speech"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï gTTS")
    print("=" * 50)
    
    try:
        from gtts import gTTS
        import io
        
        results = []
        output_dir = Path("gtts_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            resources_before = measure_resource_usage()
            
            synth_start = time.time()
            output_path = output_dir / f"gtts_{i+1}.wav"
            
            # –°–æ–∑–¥–∞–Ω–∏–µ gTTS –æ–±—ä–µ–∫—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
            tts = gTTS(text=text, lang='en', slow=False)
            tts.save(str(output_path))
            
            synth_time = time.time() - synth_start
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'gTTS',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': get_audio_length(str(output_path))
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            time.sleep(1)  # –ò–∑–±–µ–≥–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ gTTS: {e}")
        return []

def test_coqui_tts():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Coqui TTS –µ—Å–ª–∏ –æ–Ω —É—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï COQUI TTS")
    print("=" * 50)
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–º–ø–æ—Ä—Ç–∞
        try:
            from TTS.api import TTS
            tts_available = True
        except ImportError:
            print("TTS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
            return []
        
        if not tts_available:
            return []
            
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        start_time = time.time()
        tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)
        load_time = time.time() - start_time
        
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        results = []
        output_dir = Path("coqui_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            resources_before = measure_resource_usage()
            
            synth_start = time.time()
            output_path = output_dir / f"coqui_{i+1}.wav"
            
            # –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏
            tts.tts_to_file(text=text, file_path=str(output_path))
            
            synth_time = time.time() - synth_start
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'Coqui TTS',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': get_audio_length(str(output_path))
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            time.sleep(0.5)
        
        return results
        
    except Exception as e:
        print(f"Coqui TTS –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []


def test_hf_vits():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ VITS –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Hugging Face"""
    print("=" * 50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï HUGGING FACE VITS")
    print("=" * 50)
    
    try:
        from transformers import VitsModel, AutoTokenizer
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–±–æ—Ç–∞—é—â—É—é VITS –º–æ–¥–µ–ª—å
        start_time = time.time()
        model = VitsModel.from_pretrained("facebook/mms-tts-eng")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-eng")
        load_time = time.time() - start_time
        
        model = model.to(device)
        print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        results = []
        output_dir = Path("vits_results")
        output_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"–°–∏–Ω—Ç–µ–∑ {i+1}/{len(test_texts)}: {text[:50]}...")
            
            resources_before = measure_resource_usage()
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            inputs = tokenizer(text, return_tensors="pt").to(device)
            
            synth_start = time.time()
            with torch.no_grad():
                output = model(**inputs)
                
            synth_time = time.time() - synth_start
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞—É–¥–∏–æ
            audio = output.waveform[0].cpu().numpy()
            sample_rate = model.config.sampling_rate
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
            output_path = output_dir / f"vits_{i+1}.wav"
            sf.write(str(output_path), audio, sample_rate)
            
            resources_after = measure_resource_usage()
            mos_score = calculate_mos(str(output_path))
            
            result = {
                'model': 'VITS (HF)',
                'text_id': i+1,
                'synthesis_time': synth_time,
                'mos_score': mos_score,
                'audio_file': str(output_path),
                'resources_before': resources_before,
                'resources_after': resources_after,
                'audio_length': len(audio) / sample_rate
            }
            
            results.append(result)
            print(f"–í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞: {synth_time:.2f}—Å, MOS: {mos_score:.2f}, –î–ª–∏–Ω–∞: {result['audio_length']:.2f}—Å")
            
            del inputs, output
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            time.sleep(0.5)
        
        return results
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ VITS: {e}")
        return []

def run_available_models():
    """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    all_results = []
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å
    models_to_test = [
        test_bark,           # Bark –æ—Ç Suno AI
        test_mms_tts,        # Facebook MMS
        test_speecht5,       # Microsoft SpeechT5  
        test_gtts,           # Google TTS
        test_hf_vits,        # Hugging Face VITS (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Coqui TTS)
    ]
    
    # –ï—Å–ª–∏ Coqui TTS —É—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
    try:
        from TTS.api import TTS
        models_to_test.append(test_coqui_tts)
        print("Coqui TTS –¥–æ—Å—Ç—É–ø–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")
    except:
        print("Coqui TTS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏")
    
    for model_test in models_to_test:
        try:
            print(f"\n{'='*60}")
            print(f"–ó–ê–ü–£–°–ö –¢–ï–°–¢–ê: {model_test.__name__}")
            print(f"{'='*60}")
            
            results = model_test()
            all_results.extend(results)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
            time.sleep(3)
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {model_test.__name__}: {e}")
            continue
    
    return all_results


print("–ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ú–û–î–ï–õ–ï–ô –°–ò–ù–¢–ï–ó–ê –†–ï–ß–ò")
all_results = run_available_models()

print(f"\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü–æ–ª—É—á–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results)}")


def analyze_results(results):
    """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    if not results:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –º–æ–¥–µ–ª—è–º
    models_data = {}
    for result in results:
        model_name = result['model']
        if model_name not in models_data:
            models_data[model_name] = []
        models_data[model_name].append(result)
    
    # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    summary = []
    for model_name, model_results in models_data.items():
        avg_synthesis_time = np.mean([r['synthesis_time'] for r in model_results])
        avg_mos = np.mean([r['mos_score'] for r in model_results])
        avg_cpu_usage = np.mean([r['resources_after']['cpu_usage'] for r in model_results])
        avg_memory_usage = np.mean([r['resources_after']['memory_usage'] for r in model_results])
        avg_gpu_usage = np.mean([r['resources_after']['gpu_usage_gb'] for r in model_results])
        avg_audio_length = np.mean([r['audio_length'] for r in model_results])
        
        summary.append({
            'model': model_name,
            'avg_synthesis_time': avg_synthesis_time,
            'avg_mos': avg_mos,
            'avg_cpu_usage': avg_cpu_usage,
            'avg_memory_usage': avg_memory_usage,
            'avg_gpu_usage': avg_gpu_usage,
            'avg_audio_length': avg_audio_length,
            'num_samples': len(model_results)
        })
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*100)
    print("–°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*100)
    print(f"{'–ú–æ–¥–µ–ª—å':<15} {'–í—Ä–µ–º—è (—Å)':<10} {'MOS':<8} {'CPU (%)':<10} {'–ü–∞–º—è—Ç—å (%)':<12} {'GPU (GB)':<10} {'–î–ª–∏–Ω–∞ (—Å)':<10}")
    print("-"*100)
    
    for model_summary in summary:
        print(f"{model_summary['model']:<15} {model_summary['avg_synthesis_time']:<10.2f} "
              f"{model_summary['avg_mos']:<8.2f} {model_summary['avg_cpu_usage']:<10.1f} "
              f"{model_summary['avg_memory_usage']:<12.1f} {model_summary['avg_gpu_usage']:<10.2f} "
              f"{model_summary['avg_audio_length']:<10.2f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(summary) > 1:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ —Å–∏–Ω—Ç–µ–∑–∞
        models = [s['model'] for s in summary]
        times = [s['avg_synthesis_time'] for s in summary]
        bars1 = ax1.bar(models, times, color='skyblue', alpha=0.7)
        ax1.set_title('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–∞ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç')
        ax1.set_ylabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)')
        ax1.tick_params(axis='x', rotation=45)
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}', ha='center', va='bottom')
        
        # –ì—Ä–∞—Ñ–∏–∫ MOS –æ—Ü–µ–Ω–æ–∫
        mos_scores = [s['avg_mos'] for s in summary]
        bars2 = ax2.bar(models, mos_scores, color='lightgreen', alpha=0.7)
        ax2.set_title('–°—Ä–µ–¥–Ω—è—è MOS –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞')
        ax2.set_ylabel('MOS (1-5)')
        ax2.set_ylim(0, 5)
        ax2.tick_params(axis='x', rotation=45)
        for bar in bars2:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}', ha='center', va='bottom')
        
        # –ì—Ä–∞—Ñ–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CPU
        cpu_usage = [s['avg_cpu_usage'] for s in summary]
        bars3 = ax3.bar(models, cpu_usage, color='orange', alpha=0.7)
        ax3.set_title('–°—Ä–µ–¥–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU')
        ax3.set_ylabel('CPU (%)')
        ax3.tick_params(axis='x', rotation=45)
        for bar in bars3:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}', ha='center', va='bottom')
        
        # –ì—Ä–∞—Ñ–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
        gpu_usage = [s['avg_gpu_usage'] for s in summary]
        bars4 = ax4.bar(models, gpu_usage, color='red', alpha=0.7)
        ax4.set_title('–°—Ä–µ–¥–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU')
        ax4.set_ylabel('GPU (GB)')
        ax4.tick_params(axis='x', rotation=45)
        for bar in bars4:
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('results_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    return summary

summary = analyze_results(all_results)

def demonstrate_examples():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–∏–Ω—Ç–µ–∑–∞ –æ—Ç —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–ò–ú–ï–†–û–í –°–ò–ù–¢–ï–ó–ê")
    print("="*60)
    
    # –ü–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤
    model_dirs = ["bark_results", "mms_results", "speecht5_results", "gtts_results", "vits_results", "coqui_results"]
    
    for model_dir in model_dirs:
        if os.path.exists(model_dir):
            audio_files = list(Path(model_dir).glob("*.wav"))
            if audio_files:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                audio_file = audio_files[0]
                print(f"\n{model_dir}: {audio_file.name}")
                
                try:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞—É–¥–∏–æ
                    display(ipd.Audio(str(audio_file)))
                    
                    # –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞—É–¥–∏–æ
                    audio, sr = librosa.load(audio_file)
                    duration = len(audio) / sr
                    print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.2f} —Å–µ–∫, Sample rate: {sr} Hz")
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ: {e}")

demonstrate_examples()

def generate_conclusions(summary):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n" + "="*80)
    print("–í–´–í–û–î–´ –ò –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
    print("="*80)
    
    if not summary:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–≤–æ–¥–æ–≤")
        return
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é
    if len(summary) > 0:
        best_mos = max(summary, key=lambda x: x['avg_mos'])
        fastest = min(summary, key=lambda x: x['avg_synthesis_time'])
        most_efficient_cpu = min(summary, key=lambda x: x['avg_cpu_usage'])
        most_efficient_gpu = min(summary, key=lambda x: x['avg_gpu_usage'])
        
        print("–õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–û –ö–†–ò–¢–ï–†–ò–Ø–ú:")
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ –∑–≤—É–∫–∞ (MOS): {best_mos['model']} (MOS: {best_mos['avg_mos']:.2f})")
        print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å —Å–∏–Ω—Ç–µ–∑–∞: {fastest['model']} ({fastest['avg_synthesis_time']:.2f} —Å–µ–∫)")
        print(f"üíª –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å CPU: {most_efficient_cpu['model']} ({most_efficient_cpu['avg_cpu_usage']:.1f}%)")
        print(f"üéÆ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å GPU: {most_efficient_gpu['model']} ({most_efficient_gpu['avg_gpu_usage']:.2f} GB)")
        
        print("\n–¢–ï–ù–î–ï–ù–¶–ò–ò –†–ê–ó–í–ò–¢–ò–Ø TTS:")
        print("‚Ä¢ –ü–µ—Ä–µ—Ö–æ–¥ –∫ end-to-end –º–æ–¥–µ–ª—è–º (VITS, Bark)")
        print("‚Ä¢ –£–ª—É—á—à–µ–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        print("‚Ä¢ –°–Ω–∏–∂–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º")
        print("‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ (Bark)")
        print("‚Ä¢ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è multiple tasks (SpeechT5)")
        
        print("\n–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –í–´–ë–û–†–£ –ú–û–î–ï–õ–ò:")
        print("1. –î–ª—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞: –º–æ–¥–µ–ª–∏ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º–∏ MOS –æ—Ü–µ–Ω–∫–∞–º–∏")
        print("2. –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: –º–æ–¥–µ–ª–∏ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –≤—Ä–µ–º–µ–Ω–µ–º —Å–∏–Ω—Ç–µ–∑–∞")
        print("3. –î–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤: –º–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º CPU/GPU")
        print("4. –î–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏: –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Bark)")
        print("5. –î–ª—è production: —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (MMS, gTTS)")

generate_conclusions(summary)

def save_complete_report(summary, results):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
    
    report = """
# –û–¢–ß–ï–¢ –ü–û –õ–ê–ë–û–†–ê–¢–û–†–ù–û–ô –†–ê–ë–û–¢–ï ‚Ññ4
# –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏

## –í–≤–µ–¥–µ–Ω–∏–µ
–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —á–µ—Ä–µ–∑ Hugging Face Transformers –º–æ–¥–µ–ª–µ–π –∏ –¥—Ä—É–≥–∏—Ö TTS —Ä–µ—à–µ–Ω–∏–π.

## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç–µ–∫—Å—Ç–æ–≤ (10 —Ñ—Ä–∞–∑ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
- –ò–∑–º–µ—Ä—è–ª–∏—Å—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: –≤—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, GPU, –ø–∞–º—è—Ç—å)
- –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–ª–∏—Å—å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–µ MOS –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–≤—É–∫–∞
- –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

## –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
"""
    
    if summary:
        for model in summary:
            report += f"- **{model['model']}**: {model['num_samples']} samples, —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {model['avg_synthesis_time']:.2f}—Å, MOS: {model['avg_mos']:.2f}\n"
    
    report += """
## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
"""
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    if summary:
        report += "\n### –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n\n"
        report += "| –ú–æ–¥–µ–ª—å | –í—Ä–µ–º—è —Å–∏–Ω—Ç–µ–∑–∞ (—Å) | MOS | CPU (%) | –ü–∞–º—è—Ç—å (%) | GPU (GB) | –î–ª–∏–Ω–∞ (—Å) |\n"
        report += "|---------|-------------------|-----|---------|------------|----------|------------|\n"
        
        for model_summary in summary:
            report += (f"| {model_summary['model']} | {model_summary['avg_synthesis_time']:.2f} | "
                      f"{model_summary['avg_mos']:.2f} | {model_summary['avg_cpu_usage']:.1f} | "
                      f"{model_summary['avg_memory_usage']:.1f} | {model_summary['avg_gpu_usage']:.2f} | "
                      f"{model_summary['avg_audio_length']:.2f} |\n")
    
    report += """
## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
–ü—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–∏–Ω—Ç–µ–∑—É —Ä–µ—á–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. 
–ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç —Å–≤–æ–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å—Ñ–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.

### –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:
1. –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–≤—É—á–∞–Ω–∏–µ
2. –ú–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ä–µ—Å—É—Ä—Å–∞–º
3. –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞
4. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ TTS —Å–∏—Å—Ç–µ–º—ã –¥–æ—Å—Ç–∏–≥–ª–∏ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –∫–∞—á–µ—Å—Ç–≤–∞
"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open("lab4_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: lab4_report.md")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
save_complete_report(summary, all_results)

print("\n‚úÖ –õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
print("–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö")