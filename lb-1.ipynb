{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã TF-IDF + word2vec (—á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –∏ –≤—ã—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞). –¢–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞–π—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "pip install gensim scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\n",
      "–û–±—É—á–µ–Ω–∏–µ Word2Vec...\n",
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF-IDF...\n",
      "[('souffle', 0.394724577665329), ('greenway', 0.39336642622947693), ('unction', 0.3841506540775299), ('penal', 0.37015044689178467), ('feudal', 0.3689629137516022)]\n",
      "[('paris', 0.8194121718406677), ('france', 0.777579665184021), ('believe', 0.581599771976471), ('property', 0.5730543732643127), ('world', 0.5716093182563782)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# üîπ –£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "dataset_path = r\"C:\\Users\\asus\\Desktop\\ML\\dataset\\text8\"\n",
    "\n",
    "# üîπ –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# üîπ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–≤—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π –º–µ—Ç–æ–¥)\n",
    "def tokenize_text(text, method=\"regex\"):\n",
    "    text = text.lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "\n",
    "    if method == \"split\":\n",
    "        return text.split()  # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª—É)\n",
    "\n",
    "    elif method == \"regex\":\n",
    "        return re.findall(r\"\\b\\w+\\b\", text)  # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\")\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\")\n",
    "tokenized_corpus = [tokenize_text(text, method=\"regex\")]  # –í–º–µ—Å—Ç–æ word_tokenize()\n",
    "\n",
    "# üîπ –û–±—É—á–µ–Ω–∏–µ Word2Vec\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ Word2Vec...\")\n",
    "word2vec_model = gensim.models.Word2Vec(\n",
    "    sentences=tokenized_corpus, vector_size=100, window=5, min_count=5, workers=4\n",
    ")\n",
    "\n",
    "# üîπ –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è TF-IDF\n",
    "sentences = text.lower().split(\".\")[:10000]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 000 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "\n",
    "# üîπ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
    "print(\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# üîπ –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–ª–æ–∂–µ–Ω–∏—è –∏ –≤—ã—á–∏—Ç–∞–Ω–∏—è —Å–ª–æ–≤\n",
    "def word_arithmetic(word1, word2, operation=\"+\"):\n",
    "    try:\n",
    "        if operation == \"+\":\n",
    "            result_vector = word2vec_model.wv[word1] + word2vec_model.wv[word2]\n",
    "        elif operation == \"-\":\n",
    "            result_vector = word2vec_model.wv[word1] - word2vec_model.wv[word2]\n",
    "        else:\n",
    "            return \"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è\"\n",
    "        \n",
    "        similar_words = word2vec_model.wv.similar_by_vector(result_vector, topn=5)\n",
    "        return similar_words\n",
    "    except KeyError as e:\n",
    "        return f\"–°–ª–æ–≤–æ {str(e)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–æ–¥–µ–ª–∏\"\n",
    "\n",
    "# üîπ –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã\n",
    "print(word_arithmetic(\"king\", \"man\", \"-\"))  # –î–æ–ª–∂–Ω–æ –¥–∞—Ç—å —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ \"queen\"\n",
    "print(word_arithmetic(\"paris\", \"france\", \"+\"))  # –ú–æ–∂–µ—Ç –¥–∞—Ç—å \"capital\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ—Ç –≤—ã–≤–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤: **TF-IDF** –∏ **Word2Vec**.\n",
    "\n",
    "### 1. **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã TF-IDF**\n",
    "–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º **TF-IDF** (Term Frequency-Inverse Document Frequency), —á—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —Å —É—á–µ—Ç–æ–º —á–∞—Å—Ç–æ—Ç—ã –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (–∏–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞).\n",
    "\n",
    "–¢–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞) –≤ –≤—ã–≤–æ–¥–µ —Å –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ TF-IDF:\n",
    "\n",
    "```python\n",
    "[('souffle', 0.394724577665329), \n",
    " ('greenway', 0.39336642622947693), \n",
    " ('unction', 0.3841506540775299), \n",
    " ('penal', 0.37015044689178467), \n",
    " ('feudal', 0.3689629137516022)]\n",
    "```\n",
    "\n",
    "–ó–¥–µ—Å—å, –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–º–µ—Å—Ç–µ —Å —á–∏—Å–ª–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ–≥–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—É—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å:\n",
    "\n",
    "- **TF (Term Frequency)** ‚Äî —ç—Ç–æ —á–∞—Å—Ç–æ—Ç–∞, —Å –∫–æ—Ç–æ—Ä–æ–π —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.\n",
    "- **IDF (Inverse Document Frequency)** ‚Äî —ç—Ç–æ –º–µ—Ä–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤–æ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –∏ —á–µ–º —Ä–µ–∂–µ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è, —Ç–µ–º –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º –æ–Ω–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.\n",
    "\n",
    "–ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ TF-IDF, —Ç–µ–º –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–º —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ).\n",
    "\n",
    "- –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–æ **\"souffle\"** –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ TF-IDF ‚Äî 0.3947, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –µ–≥–æ –≤—ã—Å–æ–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ.\n",
    "- –°–ª–æ–≤–∞ –≤—Ä–æ–¥–µ **\"greenway\"**, **\"unction\"**, **\"penal\"** –∏ **\"feudal\"** —Ç–∞–∫–∂–µ –∏–º–µ—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "### 2. **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã Word2Vec**\n",
    "–í—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å –≤—ã–≤–æ–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ **Word2Vec**, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –≤—ã—á–∏—Å–ª—è—Ç—å –∏—Ö \"—Å—Ö–æ–∂–µ—Å—Ç—å\" –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –≤—ã—á–∏—Ç–∞–Ω–∏–µ —Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–∫–æ—Ä–æ–ª—å\" - \"–º—É–∂—á–∏–Ω–∞\" + \"–∂–µ–Ω—â–∏–Ω–∞\" = \"–∫–æ—Ä–æ–ª–µ–≤–∞\").\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **Word2Vec**:\n",
    "\n",
    "```python\n",
    "[('paris', 0.8194121718406677), \n",
    " ('france', 0.777579665184021), \n",
    " ('believe', 0.581599771976471), \n",
    " ('property', 0.5730543732643127), \n",
    " ('world', 0.5716093182563782)]\n",
    "```\n",
    "\n",
    "–ó–¥–µ—Å—å —Å–ª–æ–≤–∞ **\"paris\"**, **\"france\"**, **\"believe\"**, **\"property\"**, **\"world\"** —è–≤–ª—è—é—Ç—Å—è —Å–∞–º—ã–º–∏ –±–ª–∏–∑–∫–∏–º–∏ –ø–æ —Å–º—ã—Å–ª—É –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∫ —Å–ª–æ–≤–∞–º –≤ —Ç–µ–∫—Å—Ç–µ, –∏ —á–∏—Å–ª–∞ —Ä—è–¥–æ–º —Å –Ω–∏–º–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Å—Ç–µ–ø–µ–Ω—å \"—Å—Ö–æ–∂–µ—Å—Ç–∏\" –º–µ–∂–¥—É –Ω–∏–º–∏. –ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–±–ª–∏–∂–µ –∫ 1), —Ç–µ–º –±–æ–ª–µ–µ —Å—Ö–æ–∂–∏–º–∏ —è–≤–ª—è—é—Ç—Å—è —Å–ª–æ–≤–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.\n",
    "\n",
    "- –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞ **\"paris\"** –∏ **\"france\"** –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.819 –∏ 0.777), —á—Ç–æ –Ω–µ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ \"–ü–∞—Ä–∏–∂\" –∏ \"–§—Ä–∞–Ω—Ü–∏—è\" —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω—ã –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.\n",
    "- –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ **\"believe\"**, **\"property\"** –∏ **\"world\"** –∏–º–µ—é—Ç –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏–ª–∏ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ –æ—Å—Ç–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.\n",
    "\n",
    "### –í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "- **TF-IDF** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "- **Word2Vec** –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ —Å–ª–æ–≤–∞, —Å—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.\n",
    "\n",
    "–û–±–∞ –º–µ—Ç–æ–¥–∞ –¥–∞—é—Ç –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—Å—Ç–∞—Ö –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
